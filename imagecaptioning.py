# -*- coding: utf-8 -*-
"""ImageCaptioning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DNorqVbDk-9puC_d-2TUkCyv-HlOcDo7

**Encoder - Trích xuất đặc trưng từ hình ảnh**

Sử dụng ResNet-50 để trích xuất đặc trưng
"""

import torch
import torchvision.models as models

class EncoderCNN(torch.nn.Module):
    def __init__(self, embed_size):
        super(EncoderCNN, self).__init__()
        resnet = models.resnet50(pretrained=True)
        modules = list(resnet.children())[:-1]  # Bỏ fully connected layer
        self.resnet = torch.nn.Sequential(*modules)
        self.fc = torch.nn.Linear(resnet.fc.in_features, embed_size)
        self.bn = torch.nn.BatchNorm1d(embed_size, momentum=0.01)

    def forward(self, images):
        features = self.resnet(images)
        features = features.view(features.size(0), -1)
        features = self.fc(features)
        features = self.bn(features)
        return features

"""**Decoder - Sinh caption tuân theo quy tắc**

Sử dụng LSTM để sinh caption dựa trên đặc trưng ảnh từ Encoder.
Thêm cơ chế Attention để giúp mô hình tập trung vào các vùng quan trọng trong ảnh.
"""

import torch.nn as nn

class DecoderRNN(nn.Module):
    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):
        super(DecoderRNN, self).__init__()
        self.embed = nn.Embedding(vocab_size, embed_size)
        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, vocab_size)
        self.init_weights()

    def init_weights(self):
        '''Khởi tạo trọng số'''
        self.embed.weight.data.uniform_(-0.1, 0.1)
        self.fc.weight.data.uniform_(-0.1, 0.1)

    def forward(self, features, captions):
        embeddings = self.embed(captions)
        inputs = torch.cat((features.unsqueeze(1), embeddings), 1)
        hiddens, _ = self.lstm(inputs)
        outputs = self.fc(hiddens)
        return outputs

"""**Huấn luyện mô hình**

"""

#Thiết lập tham số huấn luyện
embed_size = 256
hidden_size = 512
vocab_size = len(vocab)  # Số lượng từ trong từ điển
num_layers = 1
learning_rate = 0.001
num_epochs = 10

#Quá trình huấn luyện
import torch.optim as optim

encoder = EncoderCNN(embed_size)
decoder = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=learning_rate)

for epoch in range(num_epochs):
    for images, captions in train_loader:
        features = encoder(images)
        outputs = decoder(features, captions)
        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')

#Dự đoán caption với hình ảnh mới
def generate_caption(image):
    feature = encoder(image)
    caption = []
    word = "<start>"
    for _ in range(20):  # Giới hạn độ dài caption
        word_embedding = decoder.embed(torch.tensor([word_to_index[word]]))
        lstm_out, _ = decoder.lstm(word_embedding.unsqueeze(1))
        word_index = torch.argmax(decoder.fc(lstm_out)).item()
        word = index_to_word[word_index]
        caption.append(word)
        if word == "<end>":
            break
    return ' '.join(caption)

#Tích hợp cảnh báo rung động
def generate_vibration_signal(caption):
    if "Cảnh báo" in caption:
        return "strong_vibration"
    elif "Rẽ trái" in caption or "Rẽ phải" in caption:
        return "short_vibration"
    elif "Dừng lại ngay" in caption:
        return "continuous_vibration"
    return "no_vibration"