# -*- coding: utf-8 -*-
"""Data Processing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KxHxOCeSc9peA7FORAmUNSCs4-qJoxcR
"""

!pip install torch torchvision
!pip install transformers
!pip install datasets
!pip install opencv-python
!pip install googletrans==4.0.0-rc1

"""Tải dữ liệu hình ảnh từ MS COCO"""

import os
import urllib.request
import zipfile

# Tải tập dữ liệu MS COCO 2014 (5GB, có thể mất thời gian)
coco_url = "http://images.cocodataset.org/zips/train2014.zip"
coco_images_path = "train2014.zip"

if not os.path.exists(coco_images_path):
    print("Đang tải dữ liệu hình ảnh MS COCO...")
    urllib.request.urlretrieve(coco_url, coco_images_path)

# Giải nén dữ liệu
with zipfile.ZipFile(coco_images_path, 'r') as zip_ref:
    zip_ref.extractall("data/coco/")
print("Tải và giải nén dữ liệu hoàn tất!")

"""Tải dữ liệu caption từ MS COCO"""

#Tệp chú thích (caption) cho tập dữ liệu này được lưu trong file JSON.
import json

# Tải file chú thích
caption_url = "http://images.cocodataset.org/annotations/annotations_trainval2014.zip"
caption_path = "annotations_trainval2014.zip"

if not os.path.exists(caption_path):
    print("Đang tải dữ liệu chú thích MS COCO...")
    urllib.request.urlretrieve(caption_url, caption_path)

# Giải nén file JSON
with zipfile.ZipFile(caption_path, 'r') as zip_ref:
    zip_ref.extractall("data/coco/")
print("Tải và giải nén dữ liệu caption hoàn tất!")

# Đọc dữ liệu JSON
caption_file = "data/coco/annotations/captions_train2014.json"
with open(caption_file, 'r') as f:
    coco_data = json.load(f)

# Xem trước một số caption
for i in range(5):
    print("Image ID:", coco_data["annotations"][i]["image_id"])
    print("Caption:", coco_data["annotations"][i]["caption"])
    print("---")

"""Dịch caption sang tiếng Việt"""

from googletrans import Translator

translator = Translator()

# Dịch 5 caption đầu tiên làm ví dụ
translated_captions = []
for i in range(5):
    eng_caption = coco_data["annotations"][i]["caption"]
    vi_caption = translator.translate(eng_caption, src="en", dest="vi").text
    translated_captions.append(vi_caption)
    print(f"Eng: {eng_caption}")
    print(f"Vi : {vi_caption}")
    print("---")

"""Chuẩn hóa caption theo quy tắc"""

import random

def format_caption(caption):
    # Ví dụ quy tắc đơn giản:
    if "man" in caption or "woman" in caption:
        return f"Có một người ở phía trước."
    elif "car" in caption or "bus" in caption:
        return f"Có một phương tiện giao thông gần đây."
    elif "stairs" in caption:
        return f"⚠️ Cảnh báo! Có cầu thang phía trước, hãy cẩn thận."
    else:
        return caption  # Giữ nguyên nếu không nhận diện được

# Áp dụng quy tắc vào các caption đã dịch
formatted_captions = [format_caption(cap) for cap in translated_captions]

for i in range(5):
    print(f"Original: {translated_captions[i]}")
    print(f"Formatted: {formatted_captions[i]}")
    print("---")

"""Xây dựng từ điển (Vocabulary) và Tokenize Caption"""

from collections import Counter
import nltk
nltk.download('punkt')

# Nếu gặp lỗi với nltk, dùng underthesea
!pip install underthesea
from underthesea import word_tokenize

# Danh sách caption mẫu
formatted_captions = [
    "Có một xe buýt ở phía trước.",
    "Cảnh báo! Có chướng ngại vật nguy hiểm, hãy dừng lại ngay.",
    "Rẽ trái sau 10 mét, có vạch qua đường."
]

# Xây dựng tần suất từ
word_freq = Counter()
for caption in formatted_captions:
    tokens = word_tokenize(caption.lower())  # Dùng underthesea thay cho nltk
    word_freq.update(tokens)

# Chỉ lấy các từ xuất hiện ít nhất 2 lần
vocab = {word: i+1 for i, (word, freq) in enumerate(word_freq.items()) if freq > 1}

# Thêm token đặc biệt
vocab["<start>"] = len(vocab) + 1
vocab["<end>"] = len(vocab) + 2
vocab["<unk>"] = len(vocab) + 3

print("Số lượng từ trong từ điển:", len(vocab))
print("Một số từ trong từ điển:", list(vocab.keys())[:10])

"""Chuyển caption thành chuỗi số (token ID)"""

from underthesea import word_tokenize

def caption_to_tokens(caption, vocab):
    tokens = word_tokenize(caption.lower())  # Dùng underthesea thay nltk
    token_ids = [vocab.get(token, vocab["<unk>"]) for token in tokens]
    token_ids = [vocab["<start>"]] + token_ids + [vocab["<end>"]]
    return token_ids

# Mã hóa caption
tokenized_captions = [caption_to_tokens(cap, vocab) for cap in formatted_captions]

print("Ví dụ một caption đã mã hóa:", tokenized_captions[0])

"""Chia tập dữ liệu"""

import random

# Giả sử formatted_captions là danh sách [(image_id, caption)]
random.shuffle(formatted_captions)
train_size = int(0.8 * len(formatted_captions))
val_size = int(0.1 * len(formatted_captions))

train_data = formatted_captions[:train_size]
val_data = formatted_captions[train_size:train_size + val_size]
test_data = formatted_captions[train_size + val_size:]

print("Số lượng mẫu trong tập Train:", len(train_data))
print("Số lượng mẫu trong tập Validation:", len(val_data))
print("Số lượng mẫu trong tập Test:", len(test_data))

!apt install git

from google.colab import auth
auth.authenticate_user()

!git config --global user.email "hongvyho123@gmail.com"
!git config --global user.name "Claira"

!git clone https://github.com/PBL5-BKDN/AI.git

# Commented out IPython magic to ensure Python compatibility.
# %cd AI

!cp -r /content/data /content/AI/

!cp -r /content/sample_data /content/AI/

!ls -lah /content/AI

!git add .
!git commit -m "processed data"
!git push origin main

